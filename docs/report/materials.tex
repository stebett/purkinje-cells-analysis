\section{Materials and Methods}

\myparagraph{Paradigm} 
The data comes from 13 rats, trained on a grasping task that they performed while freely moving, connected to a multi-electrode device, specifically one or more tetrodes inserted in the cerebellar cortex of the hemisphere ipsi-lateral to the preferred paw of the rat, while a high frequency camera recorded the movements, to allow marking landmarks, specifically \emph{lift}, the moment where the rat's limb was raised from the ground, \emph{cover}, when the limb was over the target, and \emph{grasp}, when the rat first touches the target. The camera frame rate was 50Hz, so we have an uncertainty of 20ms on the exact time location of the landmarks.

The data has been then preprocessed and the spike sorted, so my work will be done on spike trains of neurons that could be either on the same tetrode or on a different one, allowing the distinction between neighbor and distant neurons behavior and covariance or synchrony.


\myparagraph{Environment choice} 
Working on neural data is one of the hardest challenges in the data analysis domain, since the data is often structured asymmetrical, for example with different numbers of neurons for trials, and this requires adequate instruments just to be able to select the data for each analysis.

To tackle this problem, it has not been used in an existing library, since one of the objectives of the work was to understand deeply how to perform efficient data analysis. 
The first problem to solve was substantially to select a given part of a spike train, giving information relative to the landmarks, the size of the resulting array, and the basic operation that were to be performed on the spike train.

My language of choice for designing appropriate instruments is Julia, a new programming language developed specifically for scientific computing. My choice was guided by three main reasons: first, Julia is very fast, and since the task that was to be coded is for its nature the most frequently used, speed is a key factor. Second, since Julia has been built with scientific computing in mind, the code to perform the kind of operations that were needed can be very compact and elegant, which is very important for robustness and reusability of the code. The third and last reason is that Julia is a multiple dispatch programming language, which is an almost unique feature that can be exploited to write modular and reusable chunks of code, making it very elastic and easily applicable to a number of situations. This can seem very abstract, but since spike train data can take different structures, depending for example on whether we want to average over trials, use single recordings or use all the data, or if we use spike times or convoluted data, multiple dispatch is a key feature that reduces greatly the amount and complexity of code needed.

\myparagraph{Convolution} 
The most basic operation that was applied on the code is convolution, either rectangular or gaussian, depending if we need to work on discrete or continuous data. Even at this most basic level, a mistake in the definition of the function can cause distortion in the data at the next stages, and can be very hard to debug. For example, when applying gaussian convolution at the limits of an array, the behavior of the convoluting function can be unexpected, and if, the extremes of the array are kept instead of feeding a bigger array to later discard them, it can cause distortions in the data that eventually lead to wrong results.

\myparagraph{Peri-Stimulus Time Histogram} 
PSTH shows graphically the neurons that change the level of activity when the subject is stimulated, or in the case of this experiment, around the movement onset. It is useful to assess the which and how many neurons correlate with the action. The firing rates are usually normalized, so that it is possible to properly distinguish abnormal activity without being confused by neurons that are naturally more active than others.


\myparagraph{Dynamics} 
The analyses performed are mostly focused on characterizing the dynamics shown by neural signals while the animal is performing a task. 

The study of the dynamics is aimed at understanding the algorithmic level of Marr, and has the potential to give insights on which operations recurrent neural networks in the brain are performing. The knowledge of those operations is in turn precious for developing artificial systems with comparable computational capabilities, and in fact it has been proposed by Hassabis \cite{hassabis2017neuroscience} that neuroscience should focus on researching the computational and algorithmic levels. 

Although Computational Neuroscience research is undoubtedly very useful for advances in Machine Learning, that is not its only purpose, so in the continuation of this work it is intended to perform some validation on simulated data, aiming to obtain insight on the implementation level, that is how the computations performed can be carried on by (simulated) biological neurons.

There are many possible approaches to tackle the study of dynamics, and it was decided to start from the most simple ones and then move towards more complex methods.

\myparagraph{Dimensionality reduction}
Since the advent of multi-electrode recordings and optical imaging has begun, neuroscience has started looking for a proper method to analyse the huge amount of data resulting from state of the art recording methods.
One key approach has been dimensionality reduction techniques, which are extensively used to explore neural data and analyse neural dynamics through the observation of trajectories.

\myparagraph{PCA} 
The most simple dimensionality reduction technique is Principal Component Analysis (PCA) , which has the advantage of being very interpretable, since it only applies linear transformation to the data to find the projection in a lower dimensional plane which maximize the variance in the data.

Unfortunately, PCA also has disadvantages, for example it is very easily confounded by a number of factors - for example very active neurons - so it is good practice to average trials and normalize the data.

\myparagraph{GPFA} 
On the other end, more advanced techniques has been developed \cite{byron2009gaussian}, and perfectioned \cite{luttinen2009variational}, such as Gaussian Process Factor Analysis, 
GPFA applies factor analysis to spike trains, reducing the dimensionality, and at the same time fits a Gaussian process on the projected low-dimensional trajectories, smoothing them.
It has the advantage to work on single trials, without the need of averaging. Furthermore, it takes into account the fact that the data is a time series, while PCA ignores that information.



